{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra: Summary\n",
    "\n",
    "In this collection we try to provide a summary of main concepts from linear algebra. This is not a tutorial to learn linear algebra. But, if you have already have a basic background from the topic but occasionally forget the definitions then perhaps this text can be helpful to refresh your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Products in a vector space\n",
    "\n",
    "In a vector space, we often use three main types of products:\n",
    "the inner product, outer product, and tensor product.\n",
    "Each has a distinct purpose and interpretation.\n",
    "\n",
    "\n",
    "### Inner Product\n",
    "\n",
    "The inner product measures how similar or aligned two vectors are.\n",
    "It gives a scalar (number) that tells how much one vector points in the direction of another.\n",
    "\n",
    "For vectors $u, v$ in a (possibly complex) vector space $V$:\n",
    "\n",
    "$\\langle u, v \\rangle = \\sum_i u_i^* v_i$\n",
    "\n",
    "where $^*$ denotes the complex conjugate.\n",
    "\n",
    "More generally, an inner product is a function\n",
    "$ \\langle \\cdot, \\cdot \\rangle : V \\times V \\to \\mathbb{C}$\n",
    "that satisfies:\n",
    "- Conjugate symmetry: $\\langle u, v \\rangle = \\overline{\\langle v, u \\rangle}$\n",
    "- Linearity: $\\langle u, a v_1 + b v_2\\rangle = a \\langle u, v_1\\rangle  + b \\langle u, v_2 \\rangle $\n",
    "- Positive definiteness: $\\langle v, v \\rangle > 0$ if $v \\neq 0$\n",
    "\n",
    "Inner products are commonly used in the following context: \n",
    "- Defines lengths, angles, and orthogonality of vectors.\n",
    "- In quantum mechanics, $\\langle \\psi | \\phi \\rangle$ measures how similar two states are (probability amplitude).\n",
    "\n",
    "Geometrically, the inner product of two vectors means the projection of one vector onto another:\n",
    "\n",
    "$\\langle u, v \\rangle = |u| |v| \\cos \\theta$ \n",
    "\n",
    "It’s like the shadow of $u$ on $v$.\n",
    "\n",
    "\n",
    "### Outer Product\n",
    "\n",
    "The outer product forms a matrix from two vectors. Instead of comparing them, it creates a linear transformation or projector.\n",
    "\n",
    "For column vectors $u, v \\in \\mathbb{C}^n$:\n",
    "\n",
    "$u \\otimes v^\\dagger = u v^\\dagger$\n",
    "\n",
    "In real spaces, this is $u v^T$.\n",
    "\n",
    "This produces an $n \\times n$ matrix whose entries are $(u_i v_j)$.\n",
    "\n",
    "Outer products are commonly used in the following context: \n",
    "- In linear algebra: builds rank-1 matrices.\n",
    "- In quantum mechanics: $\\ket{u}\\bra{v}$ is an operator projecting onto $u$.\n",
    "- Used in density matrices and projectors.\n",
    "\n",
    "Geometrically the outer product of two vectors can be interpreted as a map: it takes any vector $x$ and gives a result along $u$, scaled by how much $x$ aligns with $v$. [todo: visualise, example, elaborate]\n",
    "It represents a directional projection or stretching.\n",
    "\n",
    "\n",
    "### Tensor Product\n",
    "\n",
    "The tensor product combines two vector spaces into a larger space that contains all combinations of both.\n",
    "\n",
    "If $u \\in V$ and $v \\in W$, then: $u \\otimes v \\in V \\otimes W$\n",
    "\n",
    "The new space $V \\otimes W$ has dimension $\\dim(V) \\times \\dim(W)$ and satisfies bilinearity:\n",
    "\n",
    "$(a u_1 + b u_2) \\otimes v = a(u_1 \\otimes v) + b(u_2 \\otimes v)$\n",
    "\n",
    "and similarly in the second argument.\n",
    "\n",
    "Tensor products are commonly used in the following contexts: \n",
    "- Appears in multilinear algebra, quantum mechanics, and machine learning.\n",
    "- In quantum mechanics: $\\ket{\\psi} \\otimes \\ket{\\phi}$ is the joint state of two quantum systems.\n",
    "\n",
    "Geometrically the tensor product can be interpreted as building a new higher-dimensional space spanned by all pairwise combinations of basis vectors. It’s like forming a grid or surface from two separate lines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Norm?\n",
    "\n",
    "A vector in a two-dimensional plane can be written as  \n",
    "\n",
    "$\n",
    "\\vec{v} = \\begin{bmatrix} x \\\\ y \\end{bmatrix},\n",
    "$  \n",
    "\n",
    "where $x$ and $y$ represent its components along the horizontal and vertical directions. Geometrically, the vector can be seen as an arrow starting at the origin $(0, 0)$ and ending at the point $(x, y)$. The **norm** (or **length**) of this vector, denoted by $\\|\\vec{v}\\|$, measures how long this arrow is, regardless of its direction. It is defined as  \n",
    "$\n",
    "\\|\\vec{v}\\| = \\sqrt{x^2 + y^2}.\n",
    "$\n",
    "\n",
    "This formula comes directly from the Pythagorean theorem: the two components $x$ and $y$ form the legs of a right triangle, and the norm gives the length of the hypotenuse. Intuitively, the norm represents the straight-line distance from the origin to the point $(x, y)$ in the plane.\n",
    "\n",
    "In three-dimensional space, a vector has three components,  \n",
    "$\n",
    "\\vec{v} = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix},\n",
    "$  \n",
    "and its norm (or length) is defined analogously as  \n",
    "$\n",
    "\\|\\vec{v}\\| = \\sqrt{x^2 + y^2 + z^2}.\n",
    "$\n",
    "\n",
    "This is the natural extension of the Pythagorean theorem to three dimensions: $x$, $y$, and $z$ represent the vector’s projections on the three coordinate axes, and their squares add up to give the square of the total distance from the origin to the point $(x, y, z)$. The norm therefore quantifies the magnitude, or “size,” of the vector in space, independent of its direction.\n",
    "\n",
    "In four-dimensional space, a vector has four components,  \n",
    "$\n",
    "\\vec{v} = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ w \\end{bmatrix},\n",
    "$  \n",
    "and its norm is defined as  \n",
    "$\n",
    "\\|\\vec{v}\\| = \\sqrt{x^2 + y^2 + z^2 + w^2}.\n",
    "$  \n",
    "This formula is a direct extension of the Pythagorean theorem to four dimensions. Each component $x, y, z, w$ represents the vector’s projection on one of four mutually perpendicular axes, and their squared contributions combine to give the total squared distance from the origin.  \n",
    "\n",
    "Although we cannot visualize four dimensions directly, we can still build intuition by analogy:  \n",
    "- In 2D, the norm measures distance on a flat plane.  \n",
    "- In 3D, the norm measures distance in space by adding a depth component.  \n",
    "- In 4D, we add one more independent direction, $w$, that contributes to the total length in the same way as $x, y,$ and $z$.  \n",
    "\n",
    "You can think of the fourth coordinate as an additional “hidden” axis perpendicular to all the others. The norm $\\|\\vec{v}\\|$ still represents the straight-line distance from the origin $(0,0,0,0)$ to the point $(x,y,z,w)$, even if we cannot visualize the geometry. It remains the measure of the vector’s total magnitude — the generalized distance in four-dimensional space.\n",
    "\n",
    "For a general vector in $n$-dimensional space, we can write  \n",
    "$\\vec{v} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_n \\end{bmatrix},$  \n",
    "where each $a_i$ represents the component of the vector along the $i$-th axis.  \n",
    "\n",
    "The **norm** (or length) of this vector is defined as  \n",
    "$\\|\\vec{v}\\| = \\sqrt{a_1^2 + a_2^2 + a_3^2 + \\cdots + a_n^2}.$  \n",
    "\n",
    "If we square both sides, we get  \n",
    "$\\|\\vec{v}\\|^2 = a_1^2 + a_2^2 + a_3^2 + \\cdots + a_n^2 = \\sum_{i=1}^{n} a_i^2.$  \n",
    "\n",
    "This means that the **square of the norm** is simply the **sum of the squares of all the components**.  \n",
    "Intuitively, each component $a_i$ contributes to the overall size of the vector along its own independent direction, and the total squared length is obtained by adding up all these contributions — a direct generalization of the Pythagorean theorem to $n$ dimensions.\n",
    "\n",
    "The **inner product** is a way to multiply two vectors to get a single number that tells us **how similar or aligned** they are in direction. For two vectors  \n",
    "$\\vec{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\cdots \\\\ u_n \\end{bmatrix}$ and  \n",
    "$\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\cdots \\\\ v_n \\end{bmatrix},$  \n",
    "the inner product is defined as  \n",
    "$\\langle \\vec{u}, \\vec{v} \\rangle = u_1v_1 + u_2v_2 + \\cdots + u_nv_n.$  \n",
    "\n",
    "If the two vectors point in the **same direction**, the inner product is large and positive;  \n",
    "if they are **perpendicular**, it is zero;  \n",
    "and if they point in **opposite directions**, it becomes negative.  \n",
    "In this way, the inner product measures **how much one vector goes in the direction of another**.\n",
    "\n",
    "Now, when we take the inner product of a vector with itself,  \n",
    "$\\langle \\vec{v}, \\vec{v} \\rangle,$  \n",
    "each component of $\\vec{v}$ is multiplied by itself, giving  \n",
    "$\\langle \\vec{v}, \\vec{v} \\rangle = a_1^2 + a_2^2 + \\cdots + a_n^2.$  \n",
    "This is exactly the same expression we use to compute the **square of the norm** of a vector,  \n",
    "$\\|\\vec{v}\\|^2 = a_1^2 + a_2^2 + \\cdots + a_n^2.$  \n",
    "\n",
    "Therefore, we can write the simple and powerful relationship  \n",
    "$\\|\\vec{v}\\|^2 = \\langle \\vec{v}, \\vec{v} \\rangle.$  \n",
    "\n",
    "Intuitively, this means that the **length of a vector** is just a special case of how similar a vector is to itself.  \n",
    "The inner product measures alignment between two directions — and when both directions are the same, it becomes a measure of the vector’s own magnitude, or its **squared length**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm for vectors with complex components\n",
    "\n",
    "An **imaginary number** is a number that gives a negative result when squared.  \n",
    "The basic imaginary unit is $i$, defined by  \n",
    "$i^2 = -1.$  \n",
    "\n",
    "A **complex number** combines a real part and an imaginary part.  \n",
    "It is written as  \n",
    "$z = a + bi,$  \n",
    "where  \n",
    "- $a$ is the **real part**,  \n",
    "- $b$ is the **imaginary part**, and  \n",
    "- $i$ is the imaginary unit.  \n",
    "\n",
    "For example, $3 + 2i$ is a complex number with real part $3$ and imaginary part $2$.  \n",
    "Complex numbers can be added, multiplied, and represented as points on a plane —  \n",
    "the **complex plane**, where the horizontal axis is the real part and the vertical axis is the imaginary part.\n",
    "\n",
    "\n",
    "A **complex vector** is a vector whose components are **complex numbers** instead of just real numbers.  \n",
    "\n",
    "For example,  \n",
    "$\\vec{v} = \\begin{bmatrix} 1 + i \\\\ 2 - i \\end{bmatrix}$  \n",
    "is a complex vector with two components.  \n",
    "\n",
    "A **complex vector** is like an ordinary vector, but each of its components can include an **imaginary part** as well as a real part.  \n",
    "\n",
    "You can think of the imaginary part as adding **extra information** about how something is **rotated or shifted**.  \n",
    "For example, in physics or engineering, a complex number can describe a wave —  \n",
    "its **real part** shows the size (amplitude), and its **imaginary part** shows how it is **shifted in time or phase**.  \n",
    "\n",
    "So, when a vector has complex components, it not only tells you *how big* and *in what direction* each component is,  \n",
    "but also includes an extra piece of information — like the **timing** or **rotation** of each part.  \n",
    "\n",
    "In short, a **complex vector** is a vector that carries both **magnitude** and **phase** information in each component.\n",
    "\n",
    "### inner product of complex vectors\n",
    "\n",
    "Let’s look at what happens when we take the **dot product** of a complex vector with itself.  \n",
    "\n",
    "Suppose we have a complex vector with two components:  \n",
    "$\\vec{v} = \\begin{bmatrix} a_1 + ib_1 \\\\ a_2 + ib_2 \\end{bmatrix}.$  \n",
    "\n",
    "If we use the usual dot product (without conjugating), we would write  \n",
    "$\\vec{v} \\cdot \\vec{v} = (a_1 + ib_1)(a_1 + ib_1) + (a_2 + ib_2)(a_2 + ib_2).$  \n",
    "\n",
    "When we expand this, we get  \n",
    "$\\vec{v} \\cdot \\vec{v} = (a_1^2 - b_1^2 + 2ia_1b_1) + (a_2^2 - b_2^2 + 2ia_2b_2).$  \n",
    "\n",
    "The result is **not purely real** — it contains imaginary terms ($2ia_1b_1$ and $2ia_2b_2$).  \n",
    "This means the dot product can give a **complex number**, which doesn’t make sense as a measure of length, because a length should always be **real and positive**.\n",
    "\n",
    "For example, if we take a single complex vector  \n",
    "$\\vec{v} = \\begin{bmatrix} i \\\\ 0 \\end{bmatrix},$  \n",
    "and compute its dot product with itself, we get  \n",
    "$\\vec{v} \\cdot \\vec{v} = i \\cdot i + 0 \\cdot 0 = i^2 = -1.$  \n",
    "\n",
    "That gives a **negative (and complex) result**, even though we expect the “length squared” of a vector to be **positive**.  \n",
    "This shows that the ordinary dot product does not behave correctly for complex numbers — it doesn’t give a meaningful measure of length or similarity.\n",
    "\n",
    "To fix this, we take the **complex conjugate** of the first vector before multiplying.  \n",
    "\n",
    "A **complex conjugate** is a way of reflecting a complex number across the real axis — it changes the sign of the imaginary part.  \n",
    "\n",
    "If a complex number is  \n",
    "$z = a + ib$  \n",
    "where $a$ is the real part and $b$ is the imaginary part,  \n",
    "then its **complex conjugate** is  \n",
    "$z^* = a - ib$  \n",
    "\n",
    "In other words, you keep the real part the same and flip the sign of the imaginary part.  \n",
    "\n",
    "#### Example:\n",
    "If $z = 3 + 2i$, then $z^* = 3 - 2i.$  \n",
    "If $z = -1 - 4i$, then $z^* = -1 + 4i.$  \n",
    "\n",
    "#### Why it’s useful:\n",
    "When you multiply a complex number by its conjugate,  \n",
    "$(a + ib)(a - ib) = a^2 + b^2,$  \n",
    "the result is always a **real number**.  \n",
    "\n",
    "This property is very useful in complex vector calculations,  \n",
    "because it ensures that quantities like the **length (or norm)** of a complex vector are always **real and positive**.\n",
    "\n",
    "\n",
    "Now back to our problem of inner product of complex vectors, if we calculate the inner product of a complex vector with it's complex conjugate, then we will have:\n",
    "\n",
    "$\\langle \\vec{v}, \\vec{v} \\rangle = (a_1 - ib_1)(a_1 + ib_1) + (a_2 - ib_2)(a_2 + ib_2).$  \n",
    "\n",
    "Now, expanding this gives  \n",
    "$\\langle \\vec{v}, \\vec{v} \\rangle = (a_1^2 + b_1^2) + (a_2^2 + b_2^2),$  \n",
    "which is **real and positive** — exactly what we expect for the square of the vector’s length.\n",
    "\n",
    "So, without conjugation, $\\vec{v} \\cdot \\vec{v}$ can be complex and meaningless as a length.  \n",
    "By using the conjugate, $\\langle \\vec{v}, \\vec{v} \\rangle$, we ensure the result is real and positive,  \n",
    "making it a proper measure of the vector’s **magnitude**.\n",
    "\n",
    "The **inner product** is a general way to multiply two vectors to get a single number that measures how much they **point in the same direction**.  \n",
    "\n",
    "For two vectors  \n",
    "$\\vec{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\cdots \\\\ u_n \\end{bmatrix}$ and  \n",
    "$\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\cdots \\\\ v_n \\end{bmatrix},$  \n",
    "the **inner product** is defined as  \n",
    "\n",
    "$\\langle \\vec{u}, \\vec{v} \\rangle = u_1^*v_1 + u_2^*v_2 + \\cdots + u_n^*v_n,$  \n",
    "\n",
    "where $u_i^*$ means the **complex conjugate** of $u_i$.  \n",
    "\n",
    "- For **real vectors**, this simplifies to the familiar **dot product**:  \n",
    "  $\\langle \\vec{u}, \\vec{v} \\rangle = u_1v_1 + u_2v_2 + \\cdots + u_nv_n.$  \n",
    "- For **complex vectors**, the conjugate ensures that when we take the inner product of a vector with itself,  \n",
    "  $\\langle \\vec{v}, \\vec{v} \\rangle,$  \n",
    "  the result is always **real and positive**, representing the vector’s squared length.  \n",
    "\n",
    "This leads directly to the definition of the **norm** (or **magnitude**) of a vector.  \n",
    "The **norm** of a vector $\\vec{v}$ is defined as the square root of its inner product with itself:  \n",
    "\n",
    "$\\|\\vec{v}\\| = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle}.$  \n",
    "\n",
    "This formula works for both real and complex vectors.  \n",
    "It generalizes the idea of “length” to any vector space:  \n",
    "the norm measures **how large** or **long** a vector is,  \n",
    "while the inner product tells us **how similar** two vectors are."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
